---
title: "Quantal Response Analysis --- Example"
author: "John Maindonald, Statistics Research Associates"
date: '`r format(Sys.Date(),"%d %B %Y")`'
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quantal response analysis, functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Conventions

* `c`: centred  
* `sc`: centered and scaled   
* `A`: additive model; same slope for all species/lifestage combinations
* `X`: Different slope per species/lifestage combination

## Data 

Hawaian contemporary data, supplied by Peter Follett, will be
used for demonstrating the use of functions in the _qra_ package.
Several different styles of model will be compared.  This turns out
to be a challenging dataset with which to work.

## Check fits to species/lifestage combinations

We start by using GLM models to check out how well lines fit to
the individual species/lifestage and species/lifestage/replicate
combinations.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment=NA)
options(show.signif.stars=FALSE)
library(qra)
```

## Set up data

```{r prepareData}
HawCon <- qra::HawCon
## Change name "CommonName" to "CN", for more compact output.
CCnum <- match("CommonName", names(HawCon))
names(HawCon)[CCnum] <- "CN"
HawCon[['CN']] <- plyr::revalue(HawCon[['CN']], 
                                c("Mediterranean fruit fly"="MedFly", 
                        "Melon fly"="MelonFly"))
## trtGp will identify species & lifestage combination
## trtGpRep will identify species, lifestage, and rep
## cTime is centered version of TrtTime
## scTime is centered and scaled version of TrtTime,
## needed to get some mixed model fits to converge
HawCon <- within(HawCon, {
  trtGp <- paste0(CN,LifestageTrt, sep=":")
  trtGpRep <- paste0(CN,LifestageTrt,RepNumber)
  cTime <- scale(TrtTime, scale=FALSE)
  scTime <- scale(TrtTime)
  logitp = log((Dead+1/6)/(Live+1/6))
})
pointLab <- 1:nrow(HawCon)
```

## Plot data

The graphs now shown use a logistic transform for the $y$-scale.
Responses appear acceptably linear, after the first one or two
observations.

```{r plots, fig.width=6, fig.height=6.5, out.width="75%"}
library(ggplot2)
graphSum(df=HawCon, link="logit", logScale=FALSE,
                     dead="Dead", tot="Total", dosevar="TrtTime", Rep="RepNumber",
                     fitRep=NULL, fitPanel=NULL,
                     byFacet=~trtGp, layout=LifestageTrt~Species,
                     maint="Hawaian contemporary data",
                     xlab="Days")
```

For what follows, observations will, where at least 7 times with < 100%
mortality are available, be restricted to days 6 or later.
Where at least 5 times with < 100% mortality are available, be
restricted to days 4 or later.  The points that remain then appear,
apart from clear outliers, to be acceptably linear on a logit mortality
scale.

Alternatives that in principle might be used are:

1. Fit curves, rather than a line. This complicates calculation of
confidence intervals for LT values, requiring the development of
new code.
2. Fit a zero-inflated model.  I have not to date found any R
package that accommodates such models, with a binomial or
quasibimomial error.  There are packages that may be worth
investigating -- this becomes a research exercise.  The function
`fieller()` in this package could be fairly straightforwardedly
adapted to handle this case.

Alternative 1 will handle a wider range of cases than 2, which
models the very specific form of nonlinearity that results
from Abbott's formula type control mortality effects.  

Both these alternatives require the estimation of additional
parameters. For models that fit curves, a strategy is required
for deciding on the family of curves that will be used, and on
how the change of curve between treatment groups will be
parameterized.  For zero-inflated models, a strategy is 
required for deciding on whether zero-inflation parameters can
be assumed common across some treatment groups.

Even where sample-based control mortality estimates are available,
these will in general be too inaccurate to use as known, fixed,
control mortalities.

```{r subSet}
tab <- sapply(split(HawCon,HawCon$trtGpRep),function(x)sum(x$Dead<x$Total))
dayMin <- rep(2,nrow(HawCon))
for(gp in names(tab)){
  selRows <- with(HawCon, trtGpRep==gp)
  if(tab[gp]>=7)dayMin[selRows] <- 6   
  if(tab[gp]>=5)dayMin[selRows] <- 4
}
```

# Diagnostic plots

The following shows diagnotic plots, after fitting one line for each
species/lifestage/replicate combination.  Fitted lines are very clearly
different between replicates. As replicates are treated as fixed effects,
the models fitted here are not suitable for generalization beyond the
data used to fit the model.  The models will be used for diagnostic
purposes. 

We wish to check:

* Is the response approximately linear?
* Are residuals approximately normally distributex?
* Are points being correctly weighted?

Two types of model will be fitted --- a generalised linear (GLM) model, 
and linear model with $y = \log(\dfrac{Dead+1/6}{Live+1/6})$.

```{r HawCon, echo=FALSE, fig.width=8, fig.asp=0.36, out.width="100%", fig.cap="Diagnostic plots --- GLM X model"}
par(mfrow=c(1,3))
## Fit one line for each unique trtGp
## Fit one line for each unique trtGpRep
modX.glm <- glm(cbind(Dead,Live)~0+trtGp/cTime,
family=quasibinomial(link='logit'), data=HawCon)
keep <- with(HawCon, TrtTime>=dayMin)
modXRep.glm <- glm(cbind(Dead,Live)~0+trtGpRep/cTime,
family=quasibinomial(link='logit'), data=subset(HawCon, TrtTime>=dayMin))
## Check residual plots
plot(modXRep.glm, which=1:3, pty="s", sub.caption="", lwd=3, cex.lab=1.45,
     labels.id=pointLab[keep], cex.caption=1.25)
par(mfrow=c(1,1))
```

The "Residuals vs Fitted" plot suggests that a systematic pattern
of variation may remain after fitting the line.  Note, however,
that the fitted smooth may be misleading, given some large outliers
and the strong indication in the "Scale-Location" plot that the GLM
model is giving too much weight to points at midrange mortalities,
and too little weight to high mortality points.  The assumption of
a constant dispersion is clearly seriously wrong.  One answer to
the changes in dispersion may be to adjust the GLM weightings.  

(The scale-location plot will be close to a horizontal line if
points are being correctly weighted.  It shows reduced variation
about the line as mortalities increase.)

>

### GLM model with prior weights {-}

We investigate adjusting the weights by reversing or partially
reversing the effect of the weighting that is implicit in the use
of a generalized linear model with quasibinomial errors.

```{r correctedWts, fig.width=8, fig.asp=0.36, out.width="100%", fig.cap="GLM model, logit link, adjusted weights"}
par(mfrow=c(1,3))
p <- fitted(modXRep.glm)
pz <- asin(sqrt(p))/asin(1)
pz <- p
disp <- summary(modXRep.glm)$dispersion
priorWts <- pz^(-0)*(1-pz)^(-0.25)

keep <- with(HawCon, TrtTime>=dayMin)
modXWRep.glm <- glm(cbind(Dead,Live)~0+trtGpRep/cTime,
                   family=quasibinomial(link='logit'), 
                   data=subset(HawCon, keep), 
                   weights=priorWts)
plot(modXWRep.glm, which=1:3, pty="s", sub.caption="", lwd=3, cex.lab=1.45,
     panel=function(x,y,...)panel.smooth(x,y,span=0.8,...),
     cex.caption=1.25, labels.id = pointLab[keep])
```
Now omit point 52 and repeat the plots:

```{r omit51and52, fig.width=8, fig.asp=0.36, out.width="100%", fig.cap="GLM model, logit link, adjusted weights, omitting points 51 and 52"}
par(mfrow=c(1,3))
keepGLM <- with(HawCon, TrtTime>=dayMin&!pointLab%in%c(52))
modXRep.glm <- glm(cbind(Dead,Live)~0+trtGpRep/cTime,
                   family=quasibinomial(link='logit'), 
                   data=subset(HawCon, keepGLM))
p <- fitted(modXRep.glm)
pz <- asin(sqrt(p))/asin(1)
pz <- p
disp <- summary(modXRep.glm)$dispersion
priorWts <- (1-pz)^(-0.25)/disp
modXWRep.glm <- glm(cbind(Dead,Live)~0+trtGpRep/cTime,
                   family=quasibinomial(link='logit'), 
                   data=subset(HawCon, keepGLM), 
                   weights=priorWts)
plot(modXWRep.glm, which=1:3, pty="s", sub.caption="", lwd=3,
     cex.lab=1.45,
     panel=function(x,y,...)panel.smooth(x,y,span=0.8,...),
     cex.caption=1.25, labels.id = pointLab[keepGLM])
modXW.glm <- glm(cbind(Dead,Live)~0+trtGp/cTime,
                   family=quasibinomial(link='logit'), 
                   data=subset(HawCon, keepGLM), 
                   weights=priorWts)
``` 

The "Residuals vs Fitted" plot is not as flat as one would like.
The span for the smooth may however be set too small for this
dataset.

>
>

## Linear model with $y = \log(\dfrac{Dead+1/6}{Live+1/6})$

Again, we fit one line per replicate, using a robust fit in order
to downweight the influence of outliers.

```{r lm-model, fig.width=8, fig.asp=0.36, out.width="100%", fig.cap="lm model; y = log((Dead+1/6)/(Live+1/6))"}
par(mfrow=c(1,3))
keep <- with(HawCon, TrtTime>=dayMin)
modXRep.rlm <- MASS::rlm(log((Dead+1/6)/(Live+1/6))~
                         0+trtGpRep/cTime, maxit=80,
                         subset(HawCon,TrtTime>=dayMin))
plot(modXRep.rlm, which=1:3, sub.caption="", id.n=4,
                 labels.id=pointLab[keep])
```

Several points are identified as outliers. The following
checks the diagnostic plots that result when they are
omitted:

```{r lm-omit, fig.width=8, fig.asp=0.36, out.width="100%", fig.cap="lm model; y = log((Dead+1/6)/(Live+1/6))"}
par(mfrow=c(1,3))
keepLM <- with(HawCon, 
               TrtTime>=dayMin & !pointLab%in%c(50,54,55,89))
modXRep.rlm <- MASS::rlm(log((Dead+1/6)/(Live+1/6))~
                         0+trtGpRep/cTime, 
                         subset(HawCon, keepLM))
plot(modXRep.rlm, which=1:3, sub.caption="", 
     labels.id=pointLab[keepLM])
```

>

>

>

# Mixed model analysis

In the mixed model context, the main effect of differences
in the way that individual lines are fitted is in the
efficiency of use of the data.

##  Fit model using `rlmer`

We fit robust versions of the linear mixed model.  This allows, however, 
for a random intercept only. The model that allowed also for a random
slope generated an error. (A guess is that there were too few points
to allow a satisfactory fit.)

```{r robustLMM}
library(robustlmm)
HawCon <- as.data.frame(HawCon)
keep <- with(HawCon, TrtTime>=dayMin)
# modAdd.rlmer <- rlmer(log((Dead+1/6)/(Live+1/6))~
#                        0+trtGp+cTime+(1|trtGpRep), 
#                   data=subset(HawCon, keep))  
modX.rlmer <- rlmer(log((Dead+1/6)/(Live+1/6))~
                    0+trtGp/cTime+(1|trtGpRep), 
                    data=subset(HawCon, keep))     
```

### Diagnostic plots from robust linear mixed model

```{r lmm-plot, fig.width=3.5, fig.asp=0.9, out.width="30%"}
plot(modX.rlmer, ask=FALSE)
```

The largest random effect is a clear outlier.

## Fit model using `glmer`

The following applies the relative weightings that
were used for the fit to `trtGpRep` with
adjusted weights:


```{r fot-glmer}
formX <- cbind(Dead,Live)~ 0+trtGp/scTime+(scTime|trtGpRep)
formXi <- cbind(Dead,Live)~ 0+trtGp/scTime+(1|trtGpRep)
modX.glmer <- glmer(formX, nAGQ=0, data=subset(HawCon, keepGLM), 
                    family=binomial(link='logit'))
modXW.glmer <- glmer(formX, nAGQ=0, data=subset(HawCon, keepGLM), 
                     weights=priorWts, family=binomial(link='logit'))
## Random intercept only, no weights
modXi.glmer <- glmer(formXi, nAGQ=0, data=subset(HawCon, keepGLM), 
                    family=binomial(link='logit'))
modXWi.glmer <- glmer(formXi, nAGQ=0, data=subset(HawCon, keepGLM), 
                     weights=priorWts, family=binomial(link='logit'))
```


## Lethal Time Estimates --- Comparison

The comparison is for `X` models, with slopes that
vary between species/lifestage combinations. Models
work either with `cTime` (centered version of `TrtTime`),
or with `scTime` (centered and scaled).

1. A robust linear mixed model, fitted to $\log(\dfrac{Dead+1/6}{Alive+1/6})$.
The model has a random intercept only. The information in the data was not
sufficient to allow for the fitting of a random slope component of variance.
2. A GLMM with no adjustment for varying dispersion
    + Model modX.glmer; work with `scTime`
3. A GLMM with weight correction
    + Model `modXW.glmer`; work with `scTime`
4. A GLMM with no adjustment for varying dispersion, random intercept only
    + Model modXi.glmer; work with `scTime` 
5. A GLMM with weight correction, random intercept only
    + Model `modXWi.glmer`; work with `scTime`    
6. A GLM model that ignores fixed (and random) replicate specific effects,
with adjustment for varying dispersion (this is included largely to warn
against its use!)
    + If there are large differences between replicates,
    this may lead to a very flat line, as with this
    dataset for Medfly eggs.

## Commentary

* Model 4 is the most defendable. The version of model 1 that
allows for random intercepts would be likewise defendable,
if the fit did not (as here) fail.
* Model 5 pools different sources of variation ---
a fudge if there is any replicate-specific pattern, which
may as for Medfly eggs lead to a very flat line.  It clearly
leads to very unsatisfactory estimates for the present data.

My preference, following on from my experience in working with
disinfestation data in the past year, is Model 1. In work that
I undertook prior to around 2000, the choice was to work with
some equivalent of Model 2.  The preference may
depend somewhat on the individual dataset.  Additionally, it
may be that we were not at that time very practiced with the
use of diagnostic plots.  

A further possibility is to fit one LT value for each replicate,
then using those as the basis for further analysis.  Results may
be unsatisfactory if the accuracy of the LT99 estimates
varies widely between replicates.

## LT99 estimates and CIs: Medfly eggs

The following assume 16 degrees of freedom for the variance-covariance
matrix.  There are 8 $\times$ 3 replicates in all. For each
species/lifestage combination, two of the three degrees of freedom are
left over after estimating the species/lifestage specific intercept,
i.e., there are 8 $\times$ 2 = 16 degrees of freedom.  The same is the
case for the (random) estimate. The value 16 is then the smallest of
the degrees of freedom for the sources of variability that contribute
to the variance-covariance matrix.

The confidence interval calculations have not taken account of
the omission of outliers in the \texttt{glmer} analyses, or of
the use of robust methods in the \texttt{rlmer} analyses.  The
confidence intervals are on this account likely to be
anti-conservative.  In principle, one can bootstrap the
calculations, but a likely roadblock for the present data is that
calculations will fail for at least some of the bootstrap 
samples.

```{r extractLT}
ascale <- qra::getScaleCoef(HawCon$cTime)
abscale <- qra::getScaleCoef(HawCon$scTime)
extractLT <- function(p=0.99, ab=c(1,9), obj=list(modX.rlmer, modX.glmer,
                                       modXW.glmer, 
                                       modXi.glmer, modXWi.glmer, 
                                       modXW.glm),
            scaling=list(ascale,abscale,abscale,abscale,abscale,ascale),
                      df.t=rep(16,4),
                      labels=c("lmm (rlmer), random intercept", 
                               "glmer, uncorrected wts",
                               "glmer, corrected weights",
                               "glmer, uncorr wts, random intercept",
                               "glmer, corr wts, random intercept",
                               "GLM: Corr weights")){
blmm <- coef(summary(obj[[1]]))[ab]
vlmm <- vcov(obj[[1]])[ab,ab]
bgmm <- coef(summary(obj[[2]]))[ab]
vgmm <- vcov(obj[[2]])[ab,ab]
bgmmW <- coef(summary(obj[[3]]))[ab]
vgmmW <- vcov(obj[[3]])[ab,ab]
bgmmi <- coef(summary(obj[[4]]))[ab]
vgmmi <- vcov(obj[[4]])[ab,ab]
bgmmWi <- coef(summary(obj[[5]]))[ab]
vgmmWi <- vcov(obj[[5]])[ab,ab]
bglmAdj <- coef(summary(obj[[6]]))[ab]
vglmAdj <- vcov(obj[[6]])[ab,ab]
ests99 <- rbind(
  qra::fieller(0.99, blmm,vlmm, df.t=16, offset=ascale)[1:4],
  qra::fieller(0.99, bgmm,vgmm, df.t=16, offset=abscale)[1:4],
  qra::fieller(0.99, bgmmW,vgmmW, df.t=16, offset=abscale)[1:4],
  qra::fieller(0.99, bgmmi,vgmmi, df.t=16, offset=abscale)[1:4],
  qra::fieller(0.99, bgmmWi,vgmmWi, df.t=16, offset=abscale)[1:4],  
  qra::fieller(0.99, bglmAdj,vglmAdj, df.t=16, offset=ascale)[1:4])
row.names(ests99) <- labels
ests99
}
```

The following are LT99 and approximate confidence intervals:
```{r all-LT99}
gpNam <- substring(names(fixef(modX.rlmer))[1:8],6)
for(i in 1:8){
cat(gpNam[i],"\n")
ests99 <- extractLT(ab=c(i,i+8))
print(ests99,digits=3)
cat("\n")
}
```

## Comparison of results

The GLM model (with corrected weights) is clearly unsatisfactory.
The fitting of a random slope does in some cases, for the model
fitted using glmer, increase the width of the confidence interval.
This happens both with and without the weighting.
The same would almost certainly be the case for the rlmer model.

Use of glmer with "corrected" weights does lead to more consistent
confidence intervals than for glmer with uncorrected weights.

Models 1 (rlmer with random intercepts only) and 5 (glmer with
random intercepts only, corrected weights) give very similar
confidence intervals.  It is reasonable to expect that 
rlmer with random intercepts and slopes would give similar
results to the weighted glmer model with random intercepts and
slopes.

Model 6 is clearly unsatisfactory.

## Slope Versus Intercept Plot

```{r ab-cap}
cap5 <- "Slope versus intercept plot.  The intercept is a difference
from the mean. Contour lines have been added for constant LT99=11 days
(labeled with Es), LT99=9 days (labeled with 9s), and LT99=7 days
(labeled with 7s)."
```

Attention is limited to the linear mixed model. 90% confidence ellipses
are added.

```{r HawCon-print, fig.width=6.5, fig.height=5, out.width="75%", message=FALSE, warning=FALSE, fig.cap=cap5, eval=TRUE}
## Extract variance-covariance matrix for estimates
ellipseFun <- function(which.coef=c(3,nFits+3), center, varmat, level=0.9, nFits, dfd){
  rad <- sqrt(2 * qf(level, 2, dfd))
car::ellipse(center=center[which.coef],
             shape=varmat[which.coef,which.coef],
             radius=rad, center.pch=1, col="gray")
}
ests <- fixef(modX.rlmer)
names(ests) <- substring(names(ests),6)
vv <- as.matrix(vcov(modX.rlmer))
ab <- matrix(ests, ncol=2)
nFits <- nrow(ab)
rnam <- names(ests[1:nFits])
dimnames(ab) <- list(rnam, c("Intercept", "Slope"))
plot(ab, xlab="Intercept", ylab="Slope")
posn <- 2 + 2*(ab[,1]<median(ab[,1]))
text(ab, labels=rnam, pos=posn)
ellipseFun(which.coef=c(7,nFits+7), center=ests, varmat=vv, nFits=nFits, dfd=2*nFits)
ellipseFun(which.coef=c(3,nFits+3), center=ests, varmat=vv, nFits=nFits, dfd=2*nFits)
ellipseFun(which.coef=c(4,nFits+4), center=ests, varmat=vv, nFits=nFits, dfd=2*nFits)
ellipseFun(which.coef=c(5,nFits+5), center=ests, varmat=vv, nFits=nFits, dfd=2*nFits)
xy <- data.frame(a = pretty(ab[,1],20))
linkfun <- make.link('logit')[['linkfun']]
xy[['b11']] <- (-xy[[1]]+linkfun(0.99))/(11-ascale)
xy[['b9']] <- (-xy[[1]]+linkfun(0.99))/(9-ascale)
xy[['b7']] <- (-xy[[1]]+linkfun(0.99))/(7-ascale)
lines(xy[,c("a","b11")], lty=2, type="b", pch="E", col=2)
lines(xy[,c("a","b9")], lty=2, type="b", pch="9", col=2)
lines(xy[,c("a","b7")], lty=2, type="b", pch="7", col=2)
```

### Hotelling T^2^
The following uses a Hotelling T^2^ test to compare the means
in the cases where the two ellipses (medFly L1 and MedFly L3)
are closest to touching. The following requires careful checking.
The $p$-value should be adjusted upwards for the number of
comparisons made.
```{r T2,eval=TRUE}
## Variance-covariance matrix for difference of intercepts,
## and difference of slopes
dmat <- matrix(0, nrow=2, ncol=2*nFits)
dmat[1,c(5, nFits+5)] <- 1
dmat[2,c(4, nFits+4)] <- -1
v <- dmat%*%vv%*%t(dmat)
d <- ests[c(5,nFits+5)]-ests[c(4,nFits+4)]
## T2 statistic for comparing the two bivariate means
T2 <- as.vector(d%*%solve(v)%*%d)
## Equivalent approx F-statistic
F2 <- T2 * 2*nFits/(2*nFits-2)
print(1-pf(F2,2,2*nFits-2), digits=3)
```

On Hotelling T^2^, see, e.g.,
https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution#Statistic


